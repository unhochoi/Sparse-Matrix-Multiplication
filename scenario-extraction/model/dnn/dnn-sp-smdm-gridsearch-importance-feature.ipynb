{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450fe173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929f3b5",
   "metadata": {},
   "source": [
    "### 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab44c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/bdlab/Desktop/sparse-matrix-multiplication/scenario-extraction/d-optimal/spmm-latency-traintest/train-test-csv/nonsquare-train-1035-from-spmm-contain-todense-over-3s-1293.csv')\n",
    "test = pd.read_csv('/Users/bdlab/Desktop/sparse-matrix-multiplication/scenario-extraction/d-optimal/spmm-latency-traintest/train-test-csv/nonsquare-test-258-from-spmm-contain-todense-over-3s-1293.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd230dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "# X_train = train[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc']] \n",
    "# y_train = train['sp_smdm']\n",
    "\n",
    "# # Test\n",
    "# X_test = test[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc']] \n",
    "# y_test = test['sp_smdm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d7630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c4f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train = train[['lr','lc','rc','ld','rd','lnnz','rnnz']] \n",
    "y_train = train['sp_smdm']\n",
    "\n",
    "# Test\n",
    "X_test = test[['lr','lc','rc','ld','rd','lnnz','rnnz']] \n",
    "y_test = test['sp_smdm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27130815",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9dafbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# MinMaxScaler 객체 생성\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# 훈련데이터의 모수 분포 저장\n",
    "minmax_scaler.fit(X_train)\n",
    "\n",
    "# 훈련 데이터 스케일링\n",
    "X_train = minmax_scaler.transform(X_train)\n",
    "\n",
    "# 테스트 데이터 스케일링\n",
    "X_test = minmax_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036c165",
   "metadata": {},
   "source": [
    "### Metric 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f747aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    rmse = K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32358a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE\n",
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f346462",
   "metadata": {},
   "source": [
    "### Hyperparameter 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea020f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dense_nparams : 초기 dense layer size\n",
    "# dense_nparams = [1024]\n",
    "# # dense_layer_sizes : 사용할 dense layer size 목록\n",
    "# dense_layer_sizes = [(256,64,16,)]\n",
    "# # input_optimizer = optimizer\n",
    "# input_optimizer = [SGD, Adagrad, RMSprop, Adam, Adamax]\n",
    "# # input_kernel_initializer : 가중치 초기화 방법\n",
    "# input_kernel_initializer =  ['uniform', 'normal', \n",
    "#                             'glorot_uniform', 'glorot_normal',\n",
    "#                             'he_uniform', 'he_normal' ]\n",
    "# # input_dropout : dropout 비율\n",
    "# input_dropout = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# # input_lr : learning_rate\n",
    "# input_lr = [0.001, 0.01, 0.1, 0.2]\n",
    "\n",
    "# # hyperparameter dictionary 화\n",
    "# param_grid = dict(dense_nparams = dense_nparams,\n",
    "#                 dense_layer_sizes = dense_layer_sizes,\n",
    "#                 input_optimizer = input_optimizer,\n",
    "#                 input_kernel_initializer = input_kernel_initializer,\n",
    "#                 input_dropout = input_dropout,\n",
    "#                 input_lr = input_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da97375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dense_nparams : 초기 dense layer size\n",
    "# dense_nparams = [1024]\n",
    "# # dense_layer_sizes : 사용할 dense layer size 목록\n",
    "# dense_layer_sizes = [(256,64,16,)]\n",
    "# # input_optimizer = optimizer\n",
    "# input_optimizer = [SGD, Adagrad, RMSprop, Adam, Adamax]\n",
    "# # input_kernel_initializer : 가중치 초기화 방법\n",
    "# input_kernel_initializer =  ['he_uniform', 'he_normal']\n",
    "# # input_dropout : dropout 비율\n",
    "# input_dropout = [0, 0.1]\n",
    "# # input_lr : learning_rate\n",
    "# input_lr = [0.001, 0.01, 0.1]\n",
    "\n",
    "# # hyperparameter dictionary 화\n",
    "# param_grid = dict(dense_nparams = dense_nparams,\n",
    "#                 dense_layer_sizes = dense_layer_sizes,\n",
    "#                 input_optimizer = input_optimizer,\n",
    "#                 input_kernel_initializer = input_kernel_initializer,\n",
    "#                 input_dropout = input_dropout,\n",
    "#                 input_lr = input_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471ce60",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning 대상 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21bd26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 대상 모델 정의\n",
    "def create_model(dense_nparams, dense_layer_sizes , input_optimizer, input_kernel_initializer, input_dropout, input_lr):\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Dense(dense_nparams, activation=\"relu\", input_shape=(X_train.shape[1],), kernel_initializer=input_kernel_initializer))  \n",
    "    model.add(Dropout(input_dropout),)\n",
    "    \n",
    "    # dense_layer_sizes 만큼 layer 추가\n",
    "    for layer_size in dense_layer_sizes:\n",
    "        model.add(Dense(layer_size, activation='relu', kernel_initializer=input_kernel_initializer))\n",
    "        model.add(Dropout(input_dropout), )\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = input_optimizer(lr=input_lr)\n",
    "    \n",
    "    model.compile(optimizer = optimizer ,\n",
    "                  loss='mape',\n",
    "                  metrics=['mape',rmse])\n",
    "    return model\n",
    "\n",
    "# hyperparameter tuning 대상 모델 선언\n",
    "# 파라미터 조합 당 epochs 는 300 번\n",
    "regressor_model = KerasRegressor(build_fn=create_model, epochs=300, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defc878",
   "metadata": {},
   "source": [
    "### GridSearchCV 정의 및 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98e5ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cross_validation 정의\n",
    "# kf = KFold(random_state=30,\n",
    "#            n_splits=10, # Fold 는 10개로 지정\n",
    "#            shuffle=True\n",
    "#           )\n",
    "\n",
    "# # gridsearch 정의\n",
    "# # scoring : 검증셋의 성능을 무엇으로 측정할 것인지\n",
    "# # n_jobs : 프로세스가 시스템의 모든 코어를 사용하도록    \n",
    "# # verbose : 모든 log 출력하도록\n",
    "# grid = GridSearchCV(estimator=regressor_model, \n",
    "#                     param_grid=param_grid, \n",
    "#                     scoring = make_scorer(mean_absolute_percentage_error, greater_is_better=False),\n",
    "#                     cv = kf,\n",
    "#                     n_jobs=-1,\n",
    "#                     verbose=3)\n",
    "\n",
    "# # gridsearch 시작\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # gridesearch 결과\n",
    "# print(\"최고의 파라미터 :\", grid_result.best_params_)\n",
    "# print(\"최고 평균 정확도 : {}\".format(grid_result.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36c5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b503abaf",
   "metadata": {},
   "source": [
    "### GridSearchCV 를 통해 탐색된 최적의 Hyperparameter 를 사용해 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142a6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 정의\n",
    "# result_model = create_model(grid_result.best_params_['dense_nparams'],\n",
    "#                             grid_result.best_params_['dense_layer_sizes'],\n",
    "#                             grid_result.best_params_['input_optimizer'],\n",
    "#                             grid_result.best_params_['input_kernel_initializer'],\n",
    "#                             grid_result.best_params_['input_dropout'],\n",
    "#                             grid_result.best_params_['input_lr'])\n",
    "# # 모델 훈련\n",
    "# history = result_model.fit(X_train, \n",
    "#                 y_train,\n",
    "#                 epochs=300, \n",
    "#                 validation_split = 0.1, \n",
    "#                 verbose =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e755818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = pd.DataFrame(history.history)\n",
    "# hist['epoch'] = history.epoch\n",
    "# hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba714adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_history(history):\n",
    "#     hist = pd.DataFrame(history.history)\n",
    "#     hist['epoch'] = history.epoch\n",
    "\n",
    "#     plt.figure(figsize=(8,12))\n",
    "\n",
    "#     # mape metric\n",
    "#     plt.subplot(2,1,1)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('mape')\n",
    "#     plt.plot(hist['epoch'], hist['mape'],\n",
    "#            label='Train Error')\n",
    "#     plt.plot(hist['epoch'], hist['val_mape'],\n",
    "#            label = 'Val Error')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # rmse metric\n",
    "#     plt.subplot(2,1,2)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('rmse')\n",
    "#     plt.plot(hist['epoch'], hist['rmse'],\n",
    "#            label='Train Error')\n",
    "#     plt.plot(hist['epoch'], hist['val_rmse'],\n",
    "#            label = 'Val Error')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cbff91",
   "metadata": {},
   "source": [
    "### 예측 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34575ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 훈련데이터 예측\n",
    "# y_train_pred = result_model.predict(X_train).reshape(-1,)\n",
    "# print(\"훈련데이터 예측 mape : {}\\n\".format(mean_absolute_percentage_error(y_train,y_train_pred)))\n",
    "\n",
    "# # 테스트데이터 예측\n",
    "# y_test_pred = result_model.predict(X_test).reshape(-1,)\n",
    "# print(\"테스트데이터 예측 mape : {}\\n\".format(mean_absolute_percentage_error(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a150537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de90b3e2",
   "metadata": {},
   "source": [
    "### 고정된 Hyperparameter 를 사용해 모델링 (Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea22bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 정의\n",
    "# fix_model = create_model(512,\n",
    "#                             (128, 64, 16, 8),\n",
    "#                             Adagrad,\n",
    "#                             'he_normal',\n",
    "#                             0,\n",
    "#                             0.09)\n",
    "\n",
    "# # 에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다\n",
    "# class PrintDot(keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs):\n",
    "#         if epoch % 100 == 0: print('')\n",
    "#         #print(\"epoch : {}, logs : {}\".format(epoch,logs))\n",
    "#         print('.', end='')\n",
    "\n",
    "# # monitor는 어떤 매개변수를 볼 것인지 입니다.\n",
    "# # patience 매개변수는 성능 향상을 체크할 에포크 횟수입니다\n",
    "# # 지정된 에포크 횟수 동안 성능 향상이 없으면 자동으로 훈련이 멈춥니다.\n",
    "# early_stop = keras.callbacks.EarlyStopping(monitor='val_mape', patience=100)\n",
    "\n",
    "# EPOCHS = 1000\n",
    "\n",
    "# # 훈련 정확도와 검증 정확도 출력\n",
    "# # 에포크마다 훈련 상태를 점검하기 위해 EarlyStopping 콜백(callback)을 사용합니다.\n",
    "# history = fix_model.fit(X_train, \n",
    "#                     y_train,\n",
    "#                     epochs=EPOCHS, \n",
    "#                     validation_split = 0.1, \n",
    "#                     verbose =0, \n",
    "#                     callbacks=[early_stop, PrintDot()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "120f61a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = pd.DataFrame(history.history)\n",
    "# hist['epoch'] = history.epoch\n",
    "# hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a7efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_history(history):\n",
    "#     hist = pd.DataFrame(history.history)\n",
    "#     hist['epoch'] = history.epoch\n",
    "\n",
    "#     plt.figure(figsize=(8,12))\n",
    "\n",
    "#     # mape metric\n",
    "#     plt.subplot(2,1,1)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('mape')\n",
    "#     plt.plot(hist['epoch'], hist['mape'],\n",
    "#            label='Train Error')\n",
    "#     plt.plot(hist['epoch'], hist['val_mape'],\n",
    "#            label = 'Val Error')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # rmse metric\n",
    "#     plt.subplot(2,1,2)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('rmse')\n",
    "#     plt.plot(hist['epoch'], hist['rmse'],\n",
    "#            label='Train Error')\n",
    "#     plt.plot(hist['epoch'], hist['val_rmse'],\n",
    "#            label = 'Val Error')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec4fbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 훈련데이터 예측\n",
    "# y_train_pred = fix_model.predict(X_train).reshape(-1,)\n",
    "# print(\"훈련데이터 예측 mape : {}\\n\".format(mean_absolute_percentage_error(y_train,y_train_pred)))\n",
    "\n",
    "# # 테스트데이터 예측\n",
    "# y_test_pred = fix_model.predict(X_test).reshape(-1,)\n",
    "# print(\"테스트데이터 예측 mape : {}\\n\".format(mean_absolute_percentage_error(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f5417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11752c45",
   "metadata": {},
   "source": [
    "### Feture 중요도 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa3323a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import permutation_importance # sklearn 22 버전부터 해당\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# # MAPE\n",
    "# def mean_absolute_percentage_error(y_test, y_pred):\n",
    "#     y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "#     return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "# # fix_model : 훈련된 모델\n",
    "# # X_train : 훈련데이터 Feature\n",
    "# # y_train : 훈련데이터 Target\n",
    "# # scoring : Feature 를 Shuffler 한 뒤, 예측값과 실제값을 어떤 Metric 을 사용해 비교할지\n",
    "# # n_repeats : 특정 Feature 를 몇번 Shuffle 할 것인지\n",
    "# # random_state : 난수 고정\n",
    "# result = permutation_importance(fix_model, X_train, y_train, scoring = make_scorer(mean_absolute_percentage_error,greater_is_better=False),\n",
    "#                             n_repeats=20,\n",
    "#                             random_state=0)\n",
    "# # Feature label\n",
    "# Feature = train[['lr','lc','rc','ld','rd','lnnz','rnnz']] \n",
    "\n",
    "# # Feature 중요도를 오름차순으로 정렬한 뒤, 해당 Feature 의 index 를 저장\n",
    "# sorted_result = result.importances_mean.argsort()\n",
    "\n",
    "# # 결과를 DataFrame 화\n",
    "# importances = pd.DataFrame(result.importances_mean[sorted_result], index=Feature.columns[sorted_result]).sort_values(0, ascending=False)   \n",
    "# importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d7d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18df30b4",
   "metadata": {},
   "source": [
    "### Grid Search 를 통해 찾은 Hyperparameter 를 고정한뒤, 훈련 데이터를 섞어가며 MAPE 결과 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a51f062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".......................\n",
      "....................................................................................................\n",
      "...........\n",
      "...............................................................................................\n",
      ".................................................................................\n",
      "....................................................................................................\n",
      ".\n",
      "......................................................................................\n",
      "....................................................................................\n",
      "....................................................................................................\n",
      "..\n",
      ".....................................................................................\n",
      " [14.79791178 13.06105678 13.34333548 13.34946862 14.24886688 13.59496495\n",
      " 13.61861877 15.29562397 13.42512345 13.41557427]\n",
      "median :  13.510044202361055\n",
      "min :  13.061056781155592\n",
      "max :  15.295623970450805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mape_list = []\n",
    "\n",
    "# 모델 정의\n",
    "fix_model = create_model(512,\n",
    "                        (128, 64, 16, 8),\n",
    "                        Adagrad,\n",
    "                        'he_normal',\n",
    "                        0,\n",
    "                        0.09)\n",
    "\n",
    "# 에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        #print(\"epoch : {}, logs : {}\".format(epoch,logs))\n",
    "        print('.', end='')\n",
    "\n",
    "# monitor는 어떤 매개변수를 볼 것인지 입니다.\n",
    "# patience 매개변수는 성능 향상을 체크할 에포크 횟수입니다\n",
    "# 지정된 에포크 횟수 동안 성능 향상이 없으면 자동으로 훈련이 멈춥니다.\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_mape', patience=80)\n",
    "\n",
    "EPOCHS = 1000\n",
    "    \n",
    "for i in range(10):\n",
    "\n",
    "    # 훈련 정확도와 검증 정확도 출력\n",
    "    # 에포크마다 훈련 상태를 점검하기 위해 EarlyStopping 콜백(callback)을 사용합니다.\n",
    "    history = fix_model.fit(X_train, \n",
    "                        y_train,\n",
    "                        epochs=EPOCHS, \n",
    "                        validation_split = 0.1, \n",
    "                        verbose =0, \n",
    "                        callbacks=[early_stop, PrintDot()])\n",
    "    \n",
    "    # 테스트데이터 예측\n",
    "    y_test_pred = fix_model.predict(X_test).reshape(-1,)\n",
    "    \n",
    "    mape_list.append(mean_absolute_percentage_error(y_test,y_test_pred))\n",
    "\n",
    "mape_list = np.array(mape_list)\n",
    "print(\"\\n\",mape_list)\n",
    "print(\"median : \" , np.median(mape_list))\n",
    "print(\"min : \" , np.min(mape_list))\n",
    "print(\"max : \" , np.max(mape_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee803c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
