{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52db13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4e49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1035, 258\n",
    "train = pd.read_csv('/Users/bdlab/Desktop/sparse-matrix-multiplication/scenario-extraction/d-optimal/spmm-latency-traintest/train-test-csv/nonsquare-train-1035-from-spmm-contain-todense-over-3s-1293.csv')\n",
    "test = pd.read_csv('/Users/bdlab/Desktop/sparse-matrix-multiplication/scenario-extraction/d-optimal/spmm-latency-traintest/train-test-csv/nonsquare-test-258-from-spmm-contain-todense-over-3s-1293.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a29fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train = train[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc']] \n",
    "y_train = train['sp_smdm']\n",
    "\n",
    "# Test\n",
    "X_test = test[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc']] \n",
    "y_test = test['sp_smdm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e7c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화(Standardization), 회귀 문제에선 MinMaxScaler가 좋음\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 변형 객체 생성\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# 훈련데이터의 모수 분포 저장\n",
    "minmax_scaler.fit(X_train)\n",
    "\n",
    "# 훈련 데이터 스케일링\n",
    "X_train = minmax_scaler.transform(X_train)\n",
    "\n",
    "# 테스트 데이터의 스케일링\n",
    "X_test = minmax_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ffc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE(train metric)\n",
    "from keras import backend as K\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    rmse = K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2454bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_absolute_percentage_error(y_test, y_pred):\n",
    "#     y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "#     return np.mean(np.abs((y_test - y_pred) / y_test)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc9bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def mape_error(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "def rmse_error(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean(np.square(y_pred - y_true))) \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b6a90",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4806da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning 대상 모델 정의\n",
    "def create_model(dense_nparams, dense_layer_sizes , input_optimizer, input_kernel_initializer, input_dropout, input_lr):\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Dense(dense_nparams, activation=\"relu\", input_shape=(X_train.shape[1],), kernel_initializer=input_kernel_initializer))  \n",
    "    model.add(Dropout(input_dropout),)\n",
    "    \n",
    "    # dense_layer_sizes 만큼 layer 추가\n",
    "    for layer_size in dense_layer_sizes:\n",
    "        model.add(Dense(layer_size, activation='relu', kernel_initializer=input_kernel_initializer))\n",
    "        model.add(Dropout(input_dropout), )\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = input_optimizer(lr=input_lr)\n",
    "    \n",
    "    model.compile(optimizer = optimizer ,\n",
    "                  loss='mape',\n",
    "                  metrics=['mape',rmse])\n",
    "    \n",
    "    # 에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다\n",
    "    class PrintDot(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            if epoch % 100 == 0: print('')\n",
    "            #print(\"epoch : {}, logs : {}\".format(epoch,logs))\n",
    "            print('.', end='')\n",
    "\n",
    "    # monitor는 어떤 매개변수를 볼 것인지 입니다.\n",
    "    # patience 매개변수는 성능 향상을 체크할 에포크 횟수입니다\n",
    "    # 지정된 에포크 횟수 동안 성능 향상이 없으면 자동으로 훈련이 멈춥니다.\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_mape', patience=50)\n",
    "\n",
    "    EPOCHS = 1000\n",
    "\n",
    "    # 훈련 정확도와 검증 정확도 출력\n",
    "    # 에포크마다 훈련 상태를 점검하기 위해 EarlyStopping 콜백(callback)을 사용합니다.\n",
    "    history = model.fit(X_train, \n",
    "                        y_train,\n",
    "                        epochs=EPOCHS, \n",
    "                        validation_split = 0.1, \n",
    "                        verbose =0, \n",
    "                        callbacks=[early_stop, PrintDot()])\n",
    "    # 예측값 출력\n",
    "    y_pred = model.predict(X_test).reshape(-1,)\n",
    "    \n",
    "    # 오차율 계산\n",
    "    mape = mape_error(y_test,y_pred)\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d12ec",
   "metadata": {},
   "source": [
    "### hyperparameter 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdf8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter dictionary 화\n",
    "param_grid = {\n",
    "    # dense_nparams : 초기 dense layer size\n",
    "    'dense_nparams' : [1024],\n",
    "    # dense_layer_sizes : 사용할 dense layer size 목록\n",
    "    'dense_layer_sizes' : [(256,64,16,)],\n",
    "    # input_optimizer = optimizer\n",
    "    'input_optimizer' : [SGD, Adagrad, RMSprop, Adam, Adamax],\n",
    "    # input_kernel_initializer : 가중치 초기화 방법\n",
    "    'input_kernel_initializer' :  ['uniform', 'normal', \n",
    "                                'glorot_uniform', 'glorot_normal',\n",
    "                                'he_uniform', 'he_normal' ],\n",
    "    # input_dropout : dropout 비율\n",
    "    'input_dropout' : [0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    # input_lr : learning_rate\n",
    "    'input_lr' : [0.001, 0.01, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ab36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dense_nparams : 초기 dense layer size\n",
    "# dense_nparams = [1024]\n",
    "# # dense_layer_sizes : 사용할 dense layer size 목록\n",
    "# dense_layer_sizes = [(256,64,16,)]\n",
    "# # input_optimizer = optimizer\n",
    "# input_optimizer = [SGD, Adagrad, RMSprop, Adam, Adamax]\n",
    "# # input_kernel_initializer : 가중치 초기화 방법\n",
    "# input_kernel_initializer =  ['uniform', 'normal', \n",
    "#                             'glorot_uniform', 'glorot_normal',\n",
    "#                             'he_uniform', 'he_normal' ]\n",
    "# # input_dropout : dropout 비율\n",
    "# input_dropout = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# # input_lr : learning_rate\n",
    "# input_lr = [0.001, 0.01, 0.1, 0.2]\n",
    "\n",
    "# # hyperparameter dictionary 화\n",
    "# param_grid = dict(dense_nparams = dense_nparams,\n",
    "#                 dense_layer_sizes = dense_layer_sizes,\n",
    "#                 input_optimizer = input_optimizer,\n",
    "#                 input_kernel_initializer = input_kernel_initializer,\n",
    "#                 input_dropout = input_dropout,\n",
    "#                 input_lr = input_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c0023",
   "metadata": {},
   "source": [
    "### BO 정의 및 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33383f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-30ea73c281cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# verbose = 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# random_state : Bayesian Optimization 상의 랜덤성이 존재하는 부분을 통제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 메소드를 이용해 최대화 과정 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, pbounds, random_state, verbose, bounds_transformer)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Data structure containing the function to be optimized, the bounds of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# its domain, and a record of the evaluations we have done so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTargetSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target_func, pbounds, random_state)\u001b[0m\n\u001b[1;32m     47\u001b[0m         self._bounds = np.array(\n\u001b[1;32m     48\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbounds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#  bayesian-optimization 라이브러리의 BayesianOptimization 클래스 import\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bayesian optimization 객체 생성\n",
    "# f : 탐색 대상 함수, pbounds : hyperparameter 집합\n",
    "# verbose = 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
    "# random_state : Bayesian Optimization 상의 랜덤성이 존재하는 부분을 통제 \n",
    "bo=BayesianOptimization(f=create_model, pbounds=param_grid, verbose=2, random_state=1 )    \n",
    "\n",
    "# 메소드를 이용해 최대화 과정 수행\n",
    "# init_points :  초기 Random Search 갯수\n",
    "# n_iter : 반복 횟수 (몇개의 입력값-함숫값 점들을 확인할지! 많을 수록 정확한 값을 얻을 수 있다.)\n",
    "# acq : Acquisition Function들 중 Expected Improvement(EI) 를 사용\n",
    "# xi : exploration 강도 (기본값은 0.0)\n",
    "bo.maximize(init_points=2, n_iter=10, acq='ei', xi=0.01)\n",
    "\n",
    "# ‘iter’는 반복 회차, ‘target’은 목적 함수의 값, 나머지는 입력값을 나타냅니다. \n",
    "# 현재 회차 이전까지 조사된 함숫값들과 비교하여, 현재 회차에 최댓값이 얻어진 경우, \n",
    "# bayesian-optimization 라이브러리는 이를 자동으로 다른 색 글자로 표시하는 것을 확인할 수 있습니다\n",
    "\n",
    "# 찾은 파라미터 값 확인\n",
    "print(bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc796bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cross_validation 정의\n",
    "# kf = KFold(random_state=30,\n",
    "#            # Fold 는 10개로 지정\n",
    "#            n_splits=10,\n",
    "#            shuffle=True\n",
    "#           )\n",
    "\n",
    "# # gridsearch 정의\n",
    "# grid = GridSearchCV(estimator=regressor_model, \n",
    "#                     param_grid=param_grid, \n",
    "#                     # scoring : 검증셋의 성능을 무엇으로 측정할 것인지\n",
    "#                     scoring = make_scorer(mean_absolute_percentage_error, greater_is_better=False),\n",
    "#                     cv = kf,\n",
    "#                     # n_jobs : 프로세스가 시스템의 모든 코어를 사용하도록    \n",
    "#                     n_jobs=-1,\n",
    "#                     # 모든 log 출력하도록\n",
    "#                     verbose=3)\n",
    "\n",
    "# # gridsearch 시작\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # gridesearch 결과\n",
    "# print(\"최고의 파라미터 :\", grid_result.best_params_)\n",
    "# print(\"최고 평균 정확도 : {}\".format(grid_result.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcd33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f16380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
