{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('/Users/bdlab/Desktop/sparse-matrix-multiplication/scenario-extraction/dataset/spmm-latency-dataset/extract-dataset-using-d-optimal/dataset/nonsquare-train-1035-from-spmm-contain-todense-over-3s-1293.csv')\n",
    "test = pd.read_csv('/Users/bdlab/Desktop/sparse-matrix-multiplication/scenario-extraction/dataset/spmm-latency-dataset/extract-dataset-using-d-optimal/dataset/nonsquare-test-258-from-spmm-contain-todense-over-3s-1293.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc','ld*rd']] \n",
    "y_train = train['bz_smsm']\n",
    "\n",
    "X_test = test[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc','ld*rd']] \n",
    "y_test = test['bz_smsm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train[['lr','lc','rc','ld','rd','lnnz','rnnz']] \n",
    "# y_train = train['bz_smsm']\n",
    "\n",
    "# # Test\n",
    "# X_test = test[['lr','lc','rc','ld','rd','lnnz','rnnz']] \n",
    "# y_test = test['bz_smsm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Metric\n",
    "def mean_absolute_percentage_error(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "def custom_scoring(real, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(real, pred))\n",
    "    r2 = r2_score(real, pred)\n",
    "    mape = mean_absolute_percentage_error(real, pred)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# 탐색 대상 함수 (XGBRegressor)\n",
    "def XGB(\n",
    "max_depth,\n",
    "learning_rate, \n",
    "n_estimators, \n",
    "#min_child_weight, \n",
    "subsample,\n",
    "_lambda,\n",
    "#gamma ,\n",
    "colsample_bytree, \n",
    "#_alpha,\n",
    "silent=True, \n",
    "n_jobs=-1):\n",
    "    \n",
    "    # 모델 정의\n",
    "    model = xgb.XGBRegressor( \n",
    "objective = 'reg:squarederror',\n",
    "max_depth=int(max_depth),\n",
    "learning_rate=learning_rate,\n",
    "n_estimators=int(n_estimators),\n",
    "#min_child_weight=min_child_weight,\n",
    "subsample=subsample,\n",
    "reg_lambda=_lambda,        \n",
    "#gamma=gamma,\n",
    "colsample_bytree=colsample_bytree, \n",
    "#reg_alpha=_alpha\n",
    "n_jobs=n_jobs        \n",
    "                              )\n",
    "    \n",
    "    # bayesian optimization을 통해 파라미터를 받아\n",
    "    # Train을 Train + Validation으로 나눠 cross-validation 성능 확인\n",
    "    kfold = KFold(n_splits=5, shuffle = True, random_state=0)\n",
    "\n",
    "    # cross-validation 평균 성능 성능 확인\n",
    "    score = cross_val_score(model,\n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            cv=kfold,\n",
    "                            scoring=make_scorer(custom_scoring,greater_is_better=False),\n",
    "                            n_jobs=-1\n",
    "                           ).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  _lambda  | colsam... | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-48.0    \u001b[0m | \u001b[0m 0.4753  \u001b[0m | \u001b[0m 0.8602  \u001b[0m | \u001b[0m 0.01003 \u001b[0m | \u001b[0m 17.42   \u001b[0m | \u001b[0m 64.68   \u001b[0m | \u001b[0m 0.5462  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-32.01   \u001b[0m | \u001b[95m 0.2676  \u001b[0m | \u001b[95m 0.6728  \u001b[0m | \u001b[95m 0.1251  \u001b[0m | \u001b[95m 19.31   \u001b[0m | \u001b[95m 91.92   \u001b[0m | \u001b[95m 0.8426  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-28.41   \u001b[0m | \u001b[95m 0.4741  \u001b[0m | \u001b[95m 0.7286  \u001b[0m | \u001b[95m 0.07077 \u001b[0m | \u001b[95m 19.09   \u001b[0m | \u001b[95m 92.1    \u001b[0m | \u001b[95m 0.7691  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-31.01   \u001b[0m | \u001b[0m 0.6379  \u001b[0m | \u001b[0m 0.6592  \u001b[0m | \u001b[0m 0.2023  \u001b[0m | \u001b[0m 18.59   \u001b[0m | \u001b[0m 67.22   \u001b[0m | \u001b[0m 0.6515  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-67.5    \u001b[0m | \u001b[0m 0.5339  \u001b[0m | \u001b[0m 0.5366  \u001b[0m | \u001b[0m 0.2357  \u001b[0m | \u001b[0m 18.73   \u001b[0m | \u001b[0m 67.37   \u001b[0m | \u001b[0m 0.7283  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-35.37   \u001b[0m | \u001b[0m 0.4686  \u001b[0m | \u001b[0m 0.5611  \u001b[0m | \u001b[0m 0.2217  \u001b[0m | \u001b[0m 21.94   \u001b[0m | \u001b[0m 116.3   \u001b[0m | \u001b[0m 0.6975  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-37.48   \u001b[0m | \u001b[0m 0.9251  \u001b[0m | \u001b[0m 0.5473  \u001b[0m | \u001b[0m 0.2961  \u001b[0m | \u001b[0m 22.58   \u001b[0m | \u001b[0m 51.33   \u001b[0m | \u001b[0m 0.727   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-28.68   \u001b[0m | \u001b[0m 0.7246  \u001b[0m | \u001b[0m 0.7868  \u001b[0m | \u001b[0m 0.1054  \u001b[0m | \u001b[0m 20.68   \u001b[0m | \u001b[0m 129.7   \u001b[0m | \u001b[0m 0.5152  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-31.81   \u001b[0m | \u001b[0m 0.2816  \u001b[0m | \u001b[0m 0.6468  \u001b[0m | \u001b[0m 0.05618 \u001b[0m | \u001b[0m 17.24   \u001b[0m | \u001b[0m 131.2   \u001b[0m | \u001b[0m 0.8532  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-35.41   \u001b[0m | \u001b[0m 0.1042  \u001b[0m | \u001b[0m 0.9105  \u001b[0m | \u001b[0m 0.1586  \u001b[0m | \u001b[0m 19.29   \u001b[0m | \u001b[0m 123.8   \u001b[0m | \u001b[0m 0.9726  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-33.84   \u001b[0m | \u001b[0m 0.1307  \u001b[0m | \u001b[0m 0.8593  \u001b[0m | \u001b[0m 0.2604  \u001b[0m | \u001b[0m 17.4    \u001b[0m | \u001b[0m 134.7   \u001b[0m | \u001b[0m 0.711   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-59.49   \u001b[0m | \u001b[0m 0.9083  \u001b[0m | \u001b[0m 0.5432  \u001b[0m | \u001b[0m 0.1723  \u001b[0m | \u001b[0m 17.04   \u001b[0m | \u001b[0m 125.6   \u001b[0m | \u001b[0m 0.6189  \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-27.77   \u001b[0m | \u001b[95m 0.99    \u001b[0m | \u001b[95m 0.873   \u001b[0m | \u001b[95m 0.1035  \u001b[0m | \u001b[95m 17.99   \u001b[0m | \u001b[95m 82.61   \u001b[0m | \u001b[95m 0.7904  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-45.54   \u001b[0m | \u001b[0m 0.7825  \u001b[0m | \u001b[0m 0.6019  \u001b[0m | \u001b[0m 0.07769 \u001b[0m | \u001b[0m 19.68   \u001b[0m | \u001b[0m 126.7   \u001b[0m | \u001b[0m 0.9829  \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m-27.52   \u001b[0m | \u001b[95m 0.8865  \u001b[0m | \u001b[95m 0.9961  \u001b[0m | \u001b[95m 0.1227  \u001b[0m | \u001b[95m 21.9    \u001b[0m | \u001b[95m 129.2   \u001b[0m | \u001b[95m 0.8625  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-32.25   \u001b[0m | \u001b[0m 0.1157  \u001b[0m | \u001b[0m 0.6656  \u001b[0m | \u001b[0m 0.1451  \u001b[0m | \u001b[0m 17.3    \u001b[0m | \u001b[0m 113.7   \u001b[0m | \u001b[0m 0.6987  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-30.63   \u001b[0m | \u001b[0m 0.6041  \u001b[0m | \u001b[0m 0.7948  \u001b[0m | \u001b[0m 0.2202  \u001b[0m | \u001b[0m 21.07   \u001b[0m | \u001b[0m 93.67   \u001b[0m | \u001b[0m 0.8302  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-32.49   \u001b[0m | \u001b[0m 0.1688  \u001b[0m | \u001b[0m 0.8353  \u001b[0m | \u001b[0m 0.2104  \u001b[0m | \u001b[0m 20.49   \u001b[0m | \u001b[0m 77.99   \u001b[0m | \u001b[0m 0.8086  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-42.64   \u001b[0m | \u001b[0m 0.8485  \u001b[0m | \u001b[0m 0.5037  \u001b[0m | \u001b[0m 0.02136 \u001b[0m | \u001b[0m 21.46   \u001b[0m | \u001b[0m 59.04   \u001b[0m | \u001b[0m 0.8782  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-31.41   \u001b[0m | \u001b[0m 0.1833  \u001b[0m | \u001b[0m 0.9968  \u001b[0m | \u001b[0m 0.1442  \u001b[0m | \u001b[0m 16.11   \u001b[0m | \u001b[0m 79.57   \u001b[0m | \u001b[0m 0.5583  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-74.61   \u001b[0m | \u001b[0m 0.2403  \u001b[0m | \u001b[0m 0.5332  \u001b[0m | \u001b[0m 0.275   \u001b[0m | \u001b[0m 15.79   \u001b[0m | \u001b[0m 81.17   \u001b[0m | \u001b[0m 0.8203  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-34.24   \u001b[0m | \u001b[0m 0.4651  \u001b[0m | \u001b[0m 0.8666  \u001b[0m | \u001b[0m 0.2584  \u001b[0m | \u001b[0m 22.93   \u001b[0m | \u001b[0m 135.5   \u001b[0m | \u001b[0m 0.6249  \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m-27.45   \u001b[0m | \u001b[95m 0.113   \u001b[0m | \u001b[95m 0.8203  \u001b[0m | \u001b[95m 0.01479 \u001b[0m | \u001b[95m 22.1    \u001b[0m | \u001b[95m 143.9   \u001b[0m | \u001b[95m 0.6135  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-32.7    \u001b[0m | \u001b[0m 0.148   \u001b[0m | \u001b[0m 0.7726  \u001b[0m | \u001b[0m 0.164   \u001b[0m | \u001b[0m 15.27   \u001b[0m | \u001b[0m 78.17   \u001b[0m | \u001b[0m 0.7539  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-34.02   \u001b[0m | \u001b[0m 0.5351  \u001b[0m | \u001b[0m 0.8802  \u001b[0m | \u001b[0m 0.2452  \u001b[0m | \u001b[0m 21.25   \u001b[0m | \u001b[0m 137.6   \u001b[0m | \u001b[0m 0.856   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-65.17   \u001b[0m | \u001b[0m 0.4406  \u001b[0m | \u001b[0m 0.5414  \u001b[0m | \u001b[0m 0.1571  \u001b[0m | \u001b[0m 18.4    \u001b[0m | \u001b[0m 52.9    \u001b[0m | \u001b[0m 0.578   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-35.63   \u001b[0m | \u001b[0m 0.7341  \u001b[0m | \u001b[0m 0.5669  \u001b[0m | \u001b[0m 0.2415  \u001b[0m | \u001b[0m 20.09   \u001b[0m | \u001b[0m 111.7   \u001b[0m | \u001b[0m 0.7667  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-28.48   \u001b[0m | \u001b[0m 0.8382  \u001b[0m | \u001b[0m 0.9282  \u001b[0m | \u001b[0m 0.1299  \u001b[0m | \u001b[0m 21.54   \u001b[0m | \u001b[0m 149.5   \u001b[0m | \u001b[0m 0.8486  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-35.78   \u001b[0m | \u001b[0m 0.1095  \u001b[0m | \u001b[0m 0.8702  \u001b[0m | \u001b[0m 0.2104  \u001b[0m | \u001b[0m 20.3    \u001b[0m | \u001b[0m 144.9   \u001b[0m | \u001b[0m 0.5775  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-33.48   \u001b[0m | \u001b[0m 0.9069  \u001b[0m | \u001b[0m 0.6477  \u001b[0m | \u001b[0m 0.2675  \u001b[0m | \u001b[0m 16.25   \u001b[0m | \u001b[0m 115.1   \u001b[0m | \u001b[0m 0.6415  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-33.07   \u001b[0m | \u001b[0m 0.1545  \u001b[0m | \u001b[0m 0.7186  \u001b[0m | \u001b[0m 0.137   \u001b[0m | \u001b[0m 20.9    \u001b[0m | \u001b[0m 67.7    \u001b[0m | \u001b[0m 0.8313  \u001b[0m |\n",
      "| \u001b[95m 32      \u001b[0m | \u001b[95m-24.88   \u001b[0m | \u001b[95m 0.9672  \u001b[0m | \u001b[95m 0.9785  \u001b[0m | \u001b[95m 0.04507 \u001b[0m | \u001b[95m 16.99   \u001b[0m | \u001b[95m 62.21   \u001b[0m | \u001b[95m 0.8545  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-28.06   \u001b[0m | \u001b[0m 0.2981  \u001b[0m | \u001b[0m 0.7798  \u001b[0m | \u001b[0m 0.06879 \u001b[0m | \u001b[0m 21.72   \u001b[0m | \u001b[0m 80.1    \u001b[0m | \u001b[0m 0.5558  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-29.16   \u001b[0m | \u001b[0m 0.423   \u001b[0m | \u001b[0m 0.7271  \u001b[0m | \u001b[0m 0.0385  \u001b[0m | \u001b[0m 18.87   \u001b[0m | \u001b[0m 80.09   \u001b[0m | \u001b[0m 0.9526  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-59.52   \u001b[0m | \u001b[0m 0.3926  \u001b[0m | \u001b[0m 0.5213  \u001b[0m | \u001b[0m 0.08428 \u001b[0m | \u001b[0m 17.26   \u001b[0m | \u001b[0m 138.8   \u001b[0m | \u001b[0m 0.6896  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-45.05   \u001b[0m | \u001b[0m 0.2111  \u001b[0m | \u001b[0m 0.5818  \u001b[0m | \u001b[0m 0.1747  \u001b[0m | \u001b[0m 16.17   \u001b[0m | \u001b[0m 62.71   \u001b[0m | \u001b[0m 0.9852  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-27.69   \u001b[0m | \u001b[0m 0.9014  \u001b[0m | \u001b[0m 0.9881  \u001b[0m | \u001b[0m 0.1406  \u001b[0m | \u001b[0m 18.11   \u001b[0m | \u001b[0m 144.9   \u001b[0m | \u001b[0m 0.791   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-39.16   \u001b[0m | \u001b[0m 0.9906  \u001b[0m | \u001b[0m 0.5963  \u001b[0m | \u001b[0m 0.1066  \u001b[0m | \u001b[0m 22.68   \u001b[0m | \u001b[0m 127.6   \u001b[0m | \u001b[0m 0.6695  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-34.49   \u001b[0m | \u001b[0m 0.9907  \u001b[0m | \u001b[0m 0.583   \u001b[0m | \u001b[0m 0.02124 \u001b[0m | \u001b[0m 16.49   \u001b[0m | \u001b[0m 134.0   \u001b[0m | \u001b[0m 0.7596  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-38.64   \u001b[0m | \u001b[0m 0.8215  \u001b[0m | \u001b[0m 0.6142  \u001b[0m | \u001b[0m 0.182   \u001b[0m | \u001b[0m 20.58   \u001b[0m | \u001b[0m 102.0   \u001b[0m | \u001b[0m 0.6855  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-30.57   \u001b[0m | \u001b[0m 0.2681  \u001b[0m | \u001b[0m 0.8578  \u001b[0m | \u001b[0m 0.2152  \u001b[0m | \u001b[0m 21.24   \u001b[0m | \u001b[0m 102.2   \u001b[0m | \u001b[0m 0.6356  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-28.84   \u001b[0m | \u001b[0m 0.9937  \u001b[0m | \u001b[0m 0.6427  \u001b[0m | \u001b[0m 0.03661 \u001b[0m | \u001b[0m 20.85   \u001b[0m | \u001b[0m 139.5   \u001b[0m | \u001b[0m 0.9469  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-30.55   \u001b[0m | \u001b[0m 0.5369  \u001b[0m | \u001b[0m 0.8069  \u001b[0m | \u001b[0m 0.1569  \u001b[0m | \u001b[0m 18.19   \u001b[0m | \u001b[0m 91.72   \u001b[0m | \u001b[0m 0.9059  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-26.25   \u001b[0m | \u001b[0m 0.9181  \u001b[0m | \u001b[0m 0.7881  \u001b[0m | \u001b[0m 0.02028 \u001b[0m | \u001b[0m 15.32   \u001b[0m | \u001b[0m 143.3   \u001b[0m | \u001b[0m 0.9772  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-32.7    \u001b[0m | \u001b[0m 0.4834  \u001b[0m | \u001b[0m 0.7385  \u001b[0m | \u001b[0m 0.2008  \u001b[0m | \u001b[0m 21.9    \u001b[0m | \u001b[0m 63.95   \u001b[0m | \u001b[0m 0.9472  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-30.63   \u001b[0m | \u001b[0m 0.4476  \u001b[0m | \u001b[0m 0.9521  \u001b[0m | \u001b[0m 0.2397  \u001b[0m | \u001b[0m 20.57   \u001b[0m | \u001b[0m 93.85   \u001b[0m | \u001b[0m 0.7266  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-28.72   \u001b[0m | \u001b[0m 0.9397  \u001b[0m | \u001b[0m 0.8885  \u001b[0m | \u001b[0m 0.1261  \u001b[0m | \u001b[0m 15.12   \u001b[0m | \u001b[0m 100.4   \u001b[0m | \u001b[0m 0.6245  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-65.15   \u001b[0m | \u001b[0m 0.4492  \u001b[0m | \u001b[0m 0.5045  \u001b[0m | \u001b[0m 0.2282  \u001b[0m | \u001b[0m 16.71   \u001b[0m | \u001b[0m 143.6   \u001b[0m | \u001b[0m 0.6425  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-29.47   \u001b[0m | \u001b[0m 0.1723  \u001b[0m | \u001b[0m 0.9273  \u001b[0m | \u001b[0m 0.07784 \u001b[0m | \u001b[0m 17.16   \u001b[0m | \u001b[0m 116.8   \u001b[0m | \u001b[0m 0.74    \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-31.59   \u001b[0m | \u001b[0m 0.2579  \u001b[0m | \u001b[0m 0.7339  \u001b[0m | \u001b[0m 0.1482  \u001b[0m | \u001b[0m 15.18   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.6108  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-32.18   \u001b[0m | \u001b[0m 0.1991  \u001b[0m | \u001b[0m 0.6673  \u001b[0m | \u001b[0m 0.2264  \u001b[0m | \u001b[0m 22.55   \u001b[0m | \u001b[0m 141.3   \u001b[0m | \u001b[0m 0.8711  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-30.93   \u001b[0m | \u001b[0m 0.1305  \u001b[0m | \u001b[0m 0.6525  \u001b[0m | \u001b[0m 0.0258  \u001b[0m | \u001b[0m 19.78   \u001b[0m | \u001b[0m 128.4   \u001b[0m | \u001b[0m 0.633   \u001b[0m |\n",
      "=================================================================================================\n",
      "{'target': -24.880260281082748, 'params': {'_lambda': 0.9671716237976958, 'colsample_bytree': 0.9784517810866844, 'learning_rate': 0.04507011754186229, 'max_depth': 16.992869637868704, 'n_estimators': 62.21252826172613, 'subsample': 0.8545270296912297}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "pbounds = {\n",
    "'max_depth': (15, 23), \n",
    "'learning_rate': (0.01, 0.3),\n",
    "'n_estimators': (50, 150),\n",
    "#'min_child_weight': (0.1, 1),\n",
    "'subsample': (0.5, 1), \n",
    "'_lambda' : (0.1,1),    \n",
    "#'gamma': (0, 0.3),     \n",
    "'colsample_bytree' :(0.5, 1)\n",
    "#'_alpha' : (0,1)           \n",
    "                      }\n",
    "\n",
    "# Bayesian optimization 객체 생성\n",
    "bo=BayesianOptimization(f=XGB, pbounds=pbounds, verbose=2, random_state=1 )    \n",
    "\n",
    "# 메소드를 이용해 최대화 과정 수행 (파라미터 넣고 목적함수 값 출력하고)\n",
    "bo.maximize(init_points=2, n_iter=50, acq='ei', xi=0.01)\n",
    "\n",
    "# 뽑힌 최적의 하이퍼파라미터 값 확인\n",
    "print(\"{}\\n\".format(bo.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "\n",
    "# # Train + Valid cross-validation을 거친, 최적의 하이퍼파라미터를 사용\n",
    "# best_model = xgb.XGBRegressor(\n",
    "# objective = 'reg:squarederror',\n",
    "# max_depth=int(bo.max['params']['max_depth']),\n",
    "# learning_rate=bo.max['params']['learning_rate'],\n",
    "# n_estimators=int(bo.max['params']['n_estimators']),\n",
    "# #min_child_weight=bo.max['params']['min_child_weight'],\n",
    "# subsample=bo.max['params']['subsample'],\n",
    "# reg_lambda = bo.max['params']['_lambda'],    \n",
    "# #gamma=bo.max['params']['gamma'],\n",
    "# colsample_bytree=bo.max['params']['colsample_bytree'],\n",
    "# #reg_alpha = bo.max['params']['_alpha'],\n",
    "# n_jobs=-1\n",
    "#                              )\n",
    "# # 모델 훈련\n",
    "# best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 최적의 하이퍼파라미터 값 확인\n",
    "# print(\"{}\\n\".format(bo.max))\n",
    "\n",
    "# # 훈련데이터 예측\n",
    "# y_train_pred = best_model.predict(X_train)\n",
    "# print(\"-------- 훈련데이터 예측 --------------------------\")\n",
    "# print(\"rmse : {}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred))))\n",
    "# print(\"mape : {}%\".format(mean_absolute_percentage_error(y_train, y_train_pred)))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # 검증데이터 예측\n",
    "# print(\"-------- 검증데이터 예측 --------------------------\")\n",
    "# print(\"mape : {}%\".format(-bo.max['target']))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # 테스트데이터 예측\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# print(\"-------- 테스트데이터 예측 -------------------------\")\n",
    "# print(\"rmse : {}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "# print(\"mape : {}%\".format(mean_absolute_percentage_error(y_test, y_pred)))\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복을 통해 MAPE 결과 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.98385344 21.25032731 20.98132174 19.31500564 19.17592758 19.57358014\n",
      " 20.05220494 20.02417353 19.80080423 20.03103586]\n",
      "median :  19.912488882279092\n",
      "min :  18.983853444661335\n",
      "max :  21.25032731464962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "mape_list = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    X_train = train[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc','ld*rd']] \n",
    "    y_train = train['bz_smsm']\n",
    "\n",
    "    X_test = test[['lr','lc','rc','ld','rd','lnnz','rnnz','lr*lc','lc*rc','lr*rc','ld*rd']] \n",
    "    y_test = test['bz_smsm']\n",
    "    \n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.1)\n",
    "\n",
    "    # Train + Valid cross-validation을 거친, 최적의 하이퍼파라미터를 사용\n",
    "    best_model = xgb.XGBRegressor(\n",
    "    objective = 'reg:squarederror',\n",
    "    max_depth=int(bo.max['params']['max_depth']),\n",
    "    learning_rate=bo.max['params']['learning_rate'],\n",
    "    n_estimators=int(bo.max['params']['n_estimators']),\n",
    "    #min_child_weight=bo.max['params']['min_child_weight'],\n",
    "    subsample=bo.max['params']['subsample'],\n",
    "    reg_lambda = bo.max['params']['_lambda'],    \n",
    "    #gamma=bo.max['params']['gamma'],\n",
    "    colsample_bytree=bo.max['params']['colsample_bytree'],\n",
    "    #reg_alpha = bo.max['params']['_alpha'],\n",
    "    n_jobs=-1\n",
    "                                 )\n",
    "    # 모델 훈련\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # 테스트데이터 예측\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # MAPE 결과 추가\n",
    "    mape_list.append(mean_absolute_percentage_error(y_test, y_pred))\n",
    "\n",
    "mape_list = np.array(mape_list)\n",
    "print(mape_list)\n",
    "print(\"median : \" , np.median(mape_list))\n",
    "print(\"min : \" , np.min(mape_list))\n",
    "print(\"max : \" , np.max(mape_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mape_list = {}\n",
    "# # 예측값, 실제값을 확인하며 mape 계산 후 mape_list에 삽입 \n",
    "# for idx,value in enumerate(y_test):\n",
    "#     mape_temp = {}\n",
    "#     predicate = int(y_pred[idx])\n",
    "#     mape = abs((value - predicate) / value) * 100\n",
    "#     mape_temp['pred'] = predicate\n",
    "#     mape_temp['real'] = value\n",
    "#     mape_temp['mape'] = mape\n",
    "#     mape_list[idx] = mape_temp\n",
    "# mape_list_sort = sorted(mape_list.values(), key=lambda x:(x['mape']), reverse=True)\n",
    "# mape_list_sort  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
